{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this notebook does:\n",
    "\n",
    "Reviews for coffee shops are preprocessed and lemmatized, 3 sets of reviews are saved - lemmatized nouns, lemmatized nouns + verbs, lemmatized nouns + adjectives.  Preprocessing steps include stopword removal, and bigram identification.\n",
    "\n",
    "Businesses defined as coffee shops are in './ProcessedData/coffeeshops_withcfcutoff.csv', the output of './ProcessingRawYelpData/EDAandDataCleaning_OnlyCoffeeShops'.\n",
    "\n",
    "A table of the reviews lemmatized, processed, and raw text is saved in './ProcessedData/lemmatizedreviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/thomasyoung/Dropbox/TYInsightProject/LDA_Fitting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/thomasyoung/Dropbox/TYInsightProject'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step one directory up to access the yelp scraping function in the helper_functions module\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/thomasyoung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_lg\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import re\n",
    "\n",
    "# NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from helper_functions.nlp_helpers import sent_to_words, doc_to_words_split, remove_stopwords, make_bigrams, make_trigrams, lemmatization\n",
    "#from nlp_helpers import sent_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46077, 7)\n",
      "(46077, 30)\n",
      "(791,)\n",
      "791\n",
      "46077\n"
     ]
    }
   ],
   "source": [
    "shops = pd.read_csv('./ProcessedData/coffeeshops_withcfcutoff.csv')\n",
    "reviews = pd.read_csv('./ProcessedData/reviews_precovid_txtprocessed.csv')\n",
    "merged = pd.merge(shops,reviews,how='inner',on = ['alias'])\n",
    "print(reviews.shape)\n",
    "print(merged.shape)\n",
    "print(merged.alias.value_counts().shape)\n",
    "print(merged.alias.nunique())\n",
    "print(merged.reviewtxt.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    it was my first time to the little canal  i wa...\n",
       "1    just moved to the area and although there are ...\n",
       "2    daytime: cafe nighttime: chillest  coziest bar...\n",
       "3    i always end up in here after i go to the metr...\n",
       "4    stopped here sunday  / /  late in the day afte...\n",
       "Name: mreviewtxt_nopunc, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the regular expression library re to replace punctuation with spaces\n",
    "# Remove punctuation\n",
    "merged['mreviewtxt_nopunc'] = merged['mreviewtxt'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "# Convert the titles to lowercase\n",
    "merged['mreviewtxt_nopunc'] = merged['mreviewtxt_nopunc'].map(lambda x: x.lower())\n",
    "# Print out the first rows of papers\n",
    "merged['mreviewtxt_nopunc'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['it', 'was', 'my', 'first', 'time', 'to', 'the', 'little', 'canal', 'was', 'looking', 'for', 'an', 'iced', 'drink', 'and', 'decided', 'to', 'come', 'in', 'after', 'seeing', 'such', 'positive', 'reviews', 'the', 'interior', 'is', 'small', 'cozy', 'and', 'intimate', 'the', 'person', 'that', 'help', 'me', 'was', 'not', 'he', 'wasn', 'very', 'friendly', 'or', 'personable', 'ordered', 'an', 'iced', 'oat', 'latte', 'the', 'drink', 'was', 'good', 'nothing', 'to', 'rave', 'or', 'go', 'out', 'of', 'my', 'way', 'to', 'if', 'you', 're', 'looking', 'to', 'catch', 'up', 'with', 'friend', 'think', 'this', 'would', 'be', 'great', 'place', 'to', 'go', 'to', 'if', 'there', 'is', 'room']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = merged.mreviewtxt_nopunc.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-grams and tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords, making bigrams, and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use','good','great','love','go','always','go','order','get','say','try','nice','need','order','really','also','but','starbuck','dunkin','gregory','pret','bluestone','la colombe',\n",
    "                  'starbucks','gregorys','store','area','people','location','drink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words,stop_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops,bigram_mod)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only nouns\n",
    "data_lemmatized_nouns = lemmatization(data_words_bigrams, nlp, allowed_postags=['NOUN'])\n",
    "data_lemmatized_nounsverbs = lemmatization(data_words_bigrams, nlp, allowed_postags=['NOUN','VERB'])\n",
    "data_lemmatized_nounsadj= lemmatization(data_words_bigrams, nlp, allowed_postags=['NOUN','ADJ'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shop', 'cure', 'day', 'service', 'coffee', 'shop', 'barista', 'craft', 'coffee']\n",
      "['shop', 'cure', 'day', 'service', 'super', 'coffee', 'shop', 'barista', 'know', 'craft', 'show', 'coffee']\n",
      "['caffeine_fix', 'shop', 'cure', 'tasty', 'perfect', 'warm', 'day', 'service', 'friendly', 'quick', 'small', 'coffee', 'shop', 'barista', 'craft', 'delicious', 'coffee']\n",
      "I should first say I hope they survive because the neighborhood needs this. That said, they need to tweak things. The seats are really uncomfortable and not functional. Weird, rotating tables that I'm sure looked cool when they bought them, are also not very functional.  Not many plug outlets as well. The message they are sending is, \"please stay for 10 minutes and then leave.\"\n",
      "Classic rock music? How about some nice jazz? It's only a couple of blocks from one of the best jazz clubs in the city (Smoke) so wouldn't it be nice to hear some Miles or Coltrane while sipping your coffee? Just my two cents.\n",
      "Coffee is fine and baked goods are solid as well. \n",
      "There's growing pains with any business so I hope they can create a more comfortable environment and become a neighborhood staple. I'll be back to see.\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized_nouns[1001])\n",
    "print(data_lemmatized_nounsverbs[1001])\n",
    "print(data_lemmatized_nounsadj[1001])\n",
    "print(merged.reviewtxt[42808])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged[reviews.columns]\n",
    "merged['review_lem_noun'] = pd.Series([' '.join(i) for i in data_lemmatized_nouns])\n",
    "merged['review_lem_nounverb'] = pd.Series([' '.join(i) for i in data_lemmatized_nounsverbs])\n",
    "merged['review_lem_nounadj'] = pd.Series([' '.join(i) for i in data_lemmatized_nounsadj])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a column of reviews, where sentences are separated by '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a column of reviews, where sentences are separated by '.', but the words in the sentences have been lemmatized\n",
    "merged['reviewtxt_periodonly'] = merged.reviewtxt.copy()\n",
    "mtext_field = \"reviewtxt_periodonly\"\n",
    "df = merged\n",
    "#Removing unnecessary punctuation\n",
    "df[mtext_field] = df[mtext_field].str.replace(r\"http\", \"\")\n",
    "df[mtext_field] = df[mtext_field].str.replace(r\"@\\S+\", \"\")\n",
    "df[mtext_field] = df[mtext_field].str.replace(r\"&\", \"and\")\n",
    "df[mtext_field] = df[mtext_field].str.replace(r\"#\", \" \")\n",
    "df[mtext_field] = df[mtext_field].str.replace(r\"@\", \"at \")\n",
    "#df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\\"\\_\\n]\", \" \")\n",
    "df[mtext_field] = df[mtext_field].str.replace(r\"\\d+\", \" \")\n",
    "df[mtext_field] = df[mtext_field].str.replace(r\"`\", \"'\")\n",
    "df[mtext_field] = df[mtext_field].str.replace(r\",\", \" \")           \n",
    "df[mtext_field] = df[mtext_field].str.replace(r\"(\", \" \") \n",
    "df[mtext_field] = df[mtext_field].str.replace(r\")\", \" \") \n",
    "df[mtext_field] = df[mtext_field].str.replace(r\"?\", \" \") \n",
    "df[mtext_field] = df[mtext_field].str.replace(r\"@\", \" \") \n",
    "df[mtext_field] = df[mtext_field].str.replace(r\"_\", \" \") \n",
    "df[mtext_field] = df[mtext_field].str.replace(r\"%\", \" \") \n",
    "df[mtext_field] = df[mtext_field].str.replace(r\"$\", \" \") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = merged.reviewtxt_periodonly.values.tolist()\n",
    "data_words = list(doc_to_words_split(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words, bigram_mod)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['parser', 'ner'])\n",
    "\n",
    "#Lemmatization\n",
    "data_lemmatized_withperiod= lemmatization(data_words_bigrams, nlp, allowed_postags=['NOUN','PROPN','VERB','ADJ','PUNCT','ADV','AUX','ADP'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['review_lem_withperiod'] = pd.Series([' '.join(i) for i in data_lemmatized_withperiod])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewidx</th>\n",
       "      <th>shopidx</th>\n",
       "      <th>alias</th>\n",
       "      <th>date</th>\n",
       "      <th>revrating</th>\n",
       "      <th>reviewtxt</th>\n",
       "      <th>mreviewtxt</th>\n",
       "      <th>review_lem_noun</th>\n",
       "      <th>review_lem_nounverb</th>\n",
       "      <th>review_lem_nounadj</th>\n",
       "      <th>reviewtxt_periodonly</th>\n",
       "      <th>review_lem_withperiod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>little-canal-new-york-2</td>\n",
       "      <td>2019-12-21</td>\n",
       "      <td>3.0</td>\n",
       "      <td>It was my first time to the Little Canal.  I w...</td>\n",
       "      <td>it was my first time to the little canal.  i w...</td>\n",
       "      <td>time canal review person oat latte way catch f...</td>\n",
       "      <td>time canal look ice decide come see review per...</td>\n",
       "      <td>first time little canal positive review interi...</td>\n",
       "      <td>It was my first time to the Little Canal.  I w...</td>\n",
       "      <td>be first time to little canal . be look for ic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>little-canal-new-york-2</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Just moved to the area and although there are ...</td>\n",
       "      <td>just moved to the area and although there are ...</td>\n",
       "      <td>cafe neighborhood spot job staff vibe anxiety ...</td>\n",
       "      <td>move cafe choose stop work feel neighborhood s...</td>\n",
       "      <td>many cafe favorite friendly neighborhood spot ...</td>\n",
       "      <td>Just moved to the area and although there are ...</td>\n",
       "      <td>just move to area be many cafe choose from one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>little-canal-new-york-2</td>\n",
       "      <td>2019-12-14</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Daytime: cafe. Nighttime: chillest, coziest ba...</td>\n",
       "      <td>daytime: cafe. nighttime: chillest  coziest ba...</td>\n",
       "      <td>nighttime bar vibe vibe price bank table wait ...</td>\n",
       "      <td>nighttime bar could want come vibe stay vibe p...</td>\n",
       "      <td>nighttime cozy bar vibe vibe price bank weekni...</td>\n",
       "      <td>Daytime: cafe. Nighttime: chillest  coziest ba...</td>\n",
       "      <td>daytime cafe . nighttime chillest cozy bar cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>little-canal-new-york-2</td>\n",
       "      <td>2019-11-04</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I always end up in here after I go to the Metr...</td>\n",
       "      <td>i always end up in here after i go to the metr...</td>\n",
       "      <td>spritz commissary canal hip atmosphere pretens...</td>\n",
       "      <td>want spend spritz commissary canal chill hip a...</td>\n",
       "      <td>spritz commissary little canal hip atmosphere ...</td>\n",
       "      <td>I always end up in here after I go to the Metr...</td>\n",
       "      <td>always end up in here after go to metrograph d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>little-canal-new-york-2</td>\n",
       "      <td>2019-10-26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Stopped here Sunday 10/11/19 late in the day a...</td>\n",
       "      <td>stopped here sunday  / /  late in the day afte...</td>\n",
       "      <td>gallery beer hummus sandwich day sort way service</td>\n",
       "      <td>stop gallery open beer hummus sandwich would e...</td>\n",
       "      <td>gallery nearby fantastic beer hummus sandwich ...</td>\n",
       "      <td>Stopped here Sunday  / /  late in the day afte...</td>\n",
       "      <td>stop here sunday late in day after gallery ope...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewidx  shopidx                    alias        date  revrating  \\\n",
       "0          6        0  little-canal-new-york-2  2019-12-21        3.0   \n",
       "1          7        0  little-canal-new-york-2  2019-12-19        5.0   \n",
       "2          8        0  little-canal-new-york-2  2019-12-14        5.0   \n",
       "3          9        0  little-canal-new-york-2  2019-11-04        4.0   \n",
       "4         10        0  little-canal-new-york-2  2019-10-26        5.0   \n",
       "\n",
       "                                           reviewtxt  \\\n",
       "0  It was my first time to the Little Canal.  I w...   \n",
       "1  Just moved to the area and although there are ...   \n",
       "2  Daytime: cafe. Nighttime: chillest, coziest ba...   \n",
       "3  I always end up in here after I go to the Metr...   \n",
       "4  Stopped here Sunday 10/11/19 late in the day a...   \n",
       "\n",
       "                                          mreviewtxt  \\\n",
       "0  it was my first time to the little canal.  i w...   \n",
       "1  just moved to the area and although there are ...   \n",
       "2  daytime: cafe. nighttime: chillest  coziest ba...   \n",
       "3  i always end up in here after i go to the metr...   \n",
       "4  stopped here sunday  / /  late in the day afte...   \n",
       "\n",
       "                                     review_lem_noun  \\\n",
       "0  time canal review person oat latte way catch f...   \n",
       "1  cafe neighborhood spot job staff vibe anxiety ...   \n",
       "2  nighttime bar vibe vibe price bank table wait ...   \n",
       "3  spritz commissary canal hip atmosphere pretens...   \n",
       "4  gallery beer hummus sandwich day sort way service   \n",
       "\n",
       "                                 review_lem_nounverb  \\\n",
       "0  time canal look ice decide come see review per...   \n",
       "1  move cafe choose stop work feel neighborhood s...   \n",
       "2  nighttime bar could want come vibe stay vibe p...   \n",
       "3  want spend spritz commissary canal chill hip a...   \n",
       "4  stop gallery open beer hummus sandwich would e...   \n",
       "\n",
       "                                  review_lem_nounadj  \\\n",
       "0  first time little canal positive review interi...   \n",
       "1  many cafe favorite friendly neighborhood spot ...   \n",
       "2  nighttime cozy bar vibe vibe price bank weekni...   \n",
       "3  spritz commissary little canal hip atmosphere ...   \n",
       "4  gallery nearby fantastic beer hummus sandwich ...   \n",
       "\n",
       "                                reviewtxt_periodonly  \\\n",
       "0  It was my first time to the Little Canal.  I w...   \n",
       "1  Just moved to the area and although there are ...   \n",
       "2  Daytime: cafe. Nighttime: chillest  coziest ba...   \n",
       "3  I always end up in here after I go to the Metr...   \n",
       "4  Stopped here Sunday  / /  late in the day afte...   \n",
       "\n",
       "                               review_lem_withperiod  \n",
       "0  be first time to little canal . be look for ic...  \n",
       "1  just move to area be many cafe choose from one...  \n",
       "2  daytime cafe . nighttime chillest cozy bar cou...  \n",
       "3  always end up in here after go to metrograph d...  \n",
       "4  stop here sunday late in day after gallery ope...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged.to_csv('./ProcessedData/lemmatizedreviews.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'big fan of ninth_street . big . fan . drink be always top_notch expect huge window in front make place . full of natural_light perfect kill time work just enjoy moment with coffee . love have place close by . .'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.review_lem_withperiod[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Big fan of Ninth Street. Big. Fan. \\n\\nThe drinks are always top notch like you'd expect  but the huge windows in front make this place. It's full of natural light that's perfect to kill some time working or just enjoying the moment with your coffee. Love having this place close by.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.reviewtxt_periodonly[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
