{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this notebook does:\n",
    "\n",
    "This notebook processes reviews into sentences based on period separation. The sentences are lemmatized and saved into './ProcessedData/reviewsentences_lemmatized.csv'. This later serves as the input for assigning sentiment to LDA topics for individual sentences ('./LDA_5topic_SentimentAssignment/LDAtopicAssignment_sentences.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step one directory up to access the yelp scraping function in the helper_functions module\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's becoming our go to place for a quick bite or breakfast or coffee or when meeting friends in the neighborhood. i have never seen any kids in there and they don't have high chair or stroller parking. but the baristas are super friendly and always trying to accommodate me when i'm visiting with my daughter. for food and drinks i would give  /  for quality and taste   /  for price.\n",
      "Index(['reviewidx', 'shopidx', 'alias', 'date', 'revrating', 'reviewtxt',\n",
      "       'mreviewtxt', 'review_lem_noun', 'review_lem_nounverb',\n",
      "       'review_lem_nounadj', 'reviewtxt_periodonly', 'review_lem_withperiod',\n",
      "       'len', 'id', 'name', 'is_closed', 'review_count', 'price', 'rating',\n",
      "       'transactions', 'latitude', 'longitude', 'geometry', 'index_right',\n",
      "       'boro_code', 'boro_name', 'county_fip', 'ntacode', 'ntaname',\n",
      "       'shape_area', 'shape_leng', 'catlist', 'numcoffeemen', 'numreviews',\n",
      "       'fraccof', 'name_top5count'],\n",
      "      dtype='object')\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_csv('./ProcessedData/lemmatizedreviews.csv')\n",
    "shops = pd.read_csv('./ProcessedData/coffeeshops_withcfcutoff.csv')\n",
    "#don't forget to add length of review as a feature\n",
    "reviews['len'] = [len(wordlist) for wordlist in reviews.mreviewtxt.str.split().to_list()]\n",
    "print(reviews.mreviewtxt[15])\n",
    "merged = reviews.copy()\n",
    "merged = merged.merge(shops,how = 'inner',on = 'alias')\n",
    "merged = merged.head(30)\n",
    "len(merged)\n",
    "print(merged.columns)\n",
    "print(merged.reviewidx.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence for i in range(len(merged)) for sentence in re.split('[\\.]',merged.reviewtxt[i])]\n",
    "reviewids = [merged.reviewidx[i] for i in range(len(merged)) for sentence in re.split('[\\.]',merged.reviewtxt[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewidx</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>It was my first time to the Little Canal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>I was looking for an iced drink and decided ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>The interior is small, cozy and intimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>The person that help me was not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>He wasn't very friendly or personable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Ordered an iced oat latte ($6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>The drink was good (nothing to rave or go ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>If you're looking to catch up with a friend,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7</td>\n",
       "      <td>Just moved to the area and although there are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>\\n\\nEveryone that works there has always been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>\\n\\nThe cold brew is tasty and buzzy without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>\\n\\nLastly and equally important (maybe this i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>There I said it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>Daytime: cafe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>Nighttime: chillest, coziest bar you could want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>Come for the vibes, stay for the vibes and al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>\\n\\nI came here on a weeknight around 7 and t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    reviewidx                                           sentence\n",
       "0           6           It was my first time to the Little Canal\n",
       "1           6    I was looking for an iced drink and decided ...\n",
       "2           6           The interior is small, cozy and intimate\n",
       "3           6                    The person that help me was not\n",
       "4           6              He wasn't very friendly or personable\n",
       "5           6                      Ordered an iced oat latte ($6\n",
       "6           6                                                00)\n",
       "7           6    The drink was good (nothing to rave or go ou...\n",
       "8           6    If you're looking to catch up with a friend,...\n",
       "9           6                                                   \n",
       "10          7  Just moved to the area and although there are ...\n",
       "11          7  \\n\\nEveryone that works there has always been ...\n",
       "12          7  \\n\\nThe cold brew is tasty and buzzy without b...\n",
       "13          7  \\n\\nLastly and equally important (maybe this i...\n",
       "14          7                                    There I said it\n",
       "15          7                                                   \n",
       "16          8                                      Daytime: cafe\n",
       "17          8    Nighttime: chillest, coziest bar you could want\n",
       "18          8   Come for the vibes, stay for the vibes and al...\n",
       "19          8   \\n\\nI came here on a weeknight around 7 and t..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentdf = pd.DataFrame()\n",
    "sentdf['reviewidx'] = pd.Series(reviewids)\n",
    "sentdf['sentence'] = pd.Series(sentences)\n",
    "sentdf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [reviewidx, sentence]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.distplot(sentdf.sentence.str.len(),kde=False)\n",
    "sentdf.sentence.str.len().describe()\n",
    "print(sentdf[sentdf.sentence.str.len()==15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138, 3)\n",
      "    index  reviewidx                                           sentence\n",
      "0       0          6           It was my first time to the Little Canal\n",
      "1       1          6  I was looking for an iced drink and decided to...\n",
      "2       2          6           The interior is small, cozy and intimate\n",
      "3       3          6                    The person that help me was not\n",
      "4       4          6              He wasn't very friendly or personable\n",
      "5       5          6                      Ordered an iced oat latte ($6\n",
      "6       7          6  The drink was good (nothing to rave or go out ...\n",
      "7       8          6  If you're looking to catch up with a friend, I...\n",
      "8      10          7  Just moved to the area and although there are ...\n",
      "9      11          7  Everyone that works there has always been supe...\n",
      "10     12          7  The cold brew is tasty and buzzy without being...\n",
      "11     13          7  Lastly and equally important (maybe this is ju...\n",
      "12     14          7                                    There I said it\n",
      "13     17          8    Nighttime: chillest, coziest bar you could want\n",
      "14     18          8  Come for the vibes, stay for the vibes and als...\n",
      "15     19          8  I came here on a weeknight around 7 and the ta...\n",
      "16     20          8  Highly recommend this spot for a date, intimat...\n",
      "17     21          8                                   Love, love, love\n",
      "18     23          9  I always end up in here after I go to the Metr...\n",
      "19     24          9  Little Canal has a similarly chill and hip atm...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      [It, was, my, first, time, to, the, Little, Ca...\n",
       "1      [I, was, looking, for, an, iced, drink, and, d...\n",
       "2       [The, interior, is, small,, cozy, and, intimate]\n",
       "3                [The, person, that, help, me, was, not]\n",
       "4           [He, wasn't, very, friendly, or, personable]\n",
       "                             ...                        \n",
       "133    [I, ordered, the, egg, sandwich, which, was, w...\n",
       "134                                  [It, was, inedible]\n",
       "135    [The, egg, however, was, fluffy, and, I, like,...\n",
       "136    [Good, vibes, here, but, I, probably, won't, b...\n",
       "137                                  [-@agirlsgottaeatt]\n",
       "Name: sentence, Length: 138, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter out any sentences that are shorter than 15 characters long\n",
    "#Remove whitespace\n",
    "\n",
    "sentdf = sentdf[sentdf.sentence.str.len()>15]\n",
    "sentdf = sentdf[['reviewidx','sentence']]\n",
    "sentdf.reset_index(inplace=True)\n",
    "sentdf['sentence'] = sentdf.sentence.str.strip()\n",
    "\n",
    "print(sentdf.shape)\n",
    "print(sentdf.head(20))\n",
    "sentdf.sentence.str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was my first time to the Little Canal\n",
      "244523\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "        \n",
    "sentences = sentdf.sentence.to_list()\n",
    "print(sentences[0])\n",
    "print(len(sentences))\n",
    "data_words = list(sent_to_words(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/thomasyoung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing the reviews to include bigrams\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "#we only focus on nouns and verbs since they probably provide the most distinct topic information\n",
    "def lemmatization(texts, allowed_postags=['NOUN','PROPN','VERB','ADJ','PUNCT','ADV','AUX','ADP']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interior', 'small', 'cozy', 'intimate']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams)\n",
    "print(data_lemmatized[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244523"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a data frame of the individual sentences of each review\n",
    "lemrevs = [' '.join(revlist) for revlist in data_lemmatized]\n",
    "len(lemrevs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244523, 3)\n",
      "         index  reviewidx                                           sentence  \\\n",
      "244513  307570      52195   Heck, forget restrooms - they don't even have...   \n",
      "244514  307571      52195   And washing your hands before eating is kind ...   \n",
      "244515  307574      52195   :(\\n\\nAnd the most frustrating thing is that ...   \n",
      "244516  307576      52196                   This location cool, service wise   \n",
      "244517  307577      52196                                 I still dislike DD   \n",
      "244518  307578      52196                  That iced coffee tasted like butt   \n",
      "244519  307580      52197  It's usually not too busy in here when I come ...   \n",
      "244520  307581      52197   But then again I'm never there during rush ho...   \n",
      "244521  307582      52197   I enjoy their service the servers are all rel...   \n",
      "244522  307583      52197   Since it's a new location it looks clean and ...   \n",
      "\n",
      "                                              lemsentence  \n",
      "244513   heck forget restroom even way customer wash hand  \n",
      "244514  wash hand eat kind important touching sort sur...  \n",
      "244515  frustrating thing restroom basin washing hand ...  \n",
      "244516                         location cool service wise  \n",
      "244517                                   still dislike dd  \n",
      "244518                        iced coffee taste like butt  \n",
      "244519                             usually busy come good  \n",
      "244520                    never rush_hour maybe get lucky  \n",
      "244521             enjoy service server relatively polite  \n",
      "244522  since new location look clean new inside let h...  \n"
     ]
    }
   ],
   "source": [
    "print(sentdf.shape)\n",
    "sentdf['lemsentence'] = pd.Series(lemrevs)\n",
    "print(sentdf.tail(10))\n",
    "sentdf = sentdf[['reviewidx','sentence','lemsentence']]\n",
    "sentdf.to_csv('./ProcessedData/reviewsentences_lemmatized.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
